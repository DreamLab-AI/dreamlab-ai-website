\chapter{Compute Backend Specification}

\section{Overview}

The compute infrastructure represents the cognitive engine of our world-class immersive system, delivering real-time rendering for multiple simultaneous users while managing complex simulations, tracking data, and volumetric reconstruction.

\section{Image Generator Cluster Architecture}

\subsection{GPU Compute Requirements}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    % Node boxes
    \foreach \y in {0,2,4,6} {
        \draw[fill=dreamlabPrimary!20, rounded corners] (0,\y) rectangle (4,\y+1.5);
        \node at (2,\y+0.75) {\textbf{Compute Node \pgfmathparse{int(\y/2+1)}\pgfmathresult}};
        \foreach \x in {0.5,1.5,2.5,3.5} {
            \draw[fill=dreamlabAccent!40] (\x-0.3,\y+0.2) rectangle (\x+0.3,\y+0.5);
        }
    }

    % Interconnect
    \draw[dreamlabSecondary, ultra thick] (4.5,0) -- (4.5,7.5);
    \draw[dreamlabSecondary, ultra thick] (4.5,3.75) -- (6,3.75);
    \node[right] at (6,3.75) {\textbf{InfiniBand HDR}};

    % Labels
    \node[below] at (2,-0.5) {\small 8 GPUs per node};
    \node[right, text width=5cm] at (5,6) {\textbf{Cluster Specifications:}\\
    • 32+ NVIDIA GPUs\\
    • 1TB+ System RAM\\
    • NVLink interconnect\\
    • RDMA capability};
\end{tikzpicture}
\caption{High-performance GPU cluster architecture}
\end{figure}

\begin{requirement}{COMP-001}{Minimum 32 high-end GPUs (NVIDIA RTX 6000 Ada or H100 class)}

\begin{requirement}{COMP-002}{Support for real-time ray tracing at 4K resolution per viewpoint}

\begin{requirement}{COMP-003}{Aggregate rendering capacity: 36 viewpoints at 60Hz (6 users × 6 surfaces)}

\subsection{Node Specifications}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}lX@{}}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
GPU Configuration & 8× NVIDIA RTX 6000 Ada or A100/H100 per node \\
CPU & Dual AMD EPYC 7763 or Intel Xeon Platinum 8480+ \\
System Memory & 512GB DDR5 ECC per node minimum \\
GPU Memory & 48GB+ VRAM per GPU \\
Storage & 4× 3.84TB NVMe SSD in RAID 0 for scratch \\
Interconnect & NVLink/NVSwitch for intra-node GPU communication \\
Network & Dual 200GbE or InfiniBand HDR200 \\
\bottomrule
\end{tabularx}
\caption{Individual compute node specifications}
\end{table}

\section{High-Performance Networking}

\subsection{Internal Fabric}

\begin{requirement}{COMP-004}{Non-blocking spine-leaf architecture with <5\textmu s latency}

\begin{requirement}{COMP-005}{400Gb/s aggregate bandwidth to storage systems}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.7]
    % Spine switches
    \draw[fill=dreamlabSecondary!30] (2,6) rectangle (4,7);
    \draw[fill=dreamlabSecondary!30] (6,6) rectangle (8,7);
    \node at (3,6.5) {\small Spine};
    \node at (7,6.5) {\small Spine};

    % Leaf switches
    \foreach \x in {0,3,6,9} {
        \draw[fill=dreamlabAccent!30] (\x,3) rectangle (\x+2,4);
        \node at (\x+1,3.5) {\small Leaf};
    }

    % Connections
    \foreach \spine in {3,7} {
        \foreach \leaf in {1,4,7,10} {
            \draw[dreamlabPrimary] (\spine,6) -- (\leaf,4);
        }
    }

    % Compute nodes
    \foreach \x in {0,3,6,9} {
        \draw[fill=dreamlabPrimary!20] (\x+0.5,0) rectangle (\x+1.5,1);
        \draw[dreamlabPrimary, thick] (\x+1,1) -- (\x+1,3);
    }

    \node[below] at (5,-0.5) {\textbf{Non-blocking Fabric Design}};
\end{tikzpicture}
\caption{Network topology for zero congestion}
\end{figure}

\subsection{External Connectivity}

\begin{itemize}
    \item Dual 100GbE uplinks to campus network
    \item Dedicated 10GbE management network
    \item Out-of-band IPMI access for all nodes
    \item Provision for Science DMZ connection
\end{itemize}

\section{Storage Architecture}

\subsection{High-Performance Storage Tiers}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}lXr@{}}
\toprule
\textbf{Tier} & \textbf{Purpose} & \textbf{Capacity} \\
\midrule
Hot (NVMe) & Active rendering data, temp files & 100TB \\
Warm (SSD) & Project files, recent captures & 500TB \\
Cold (HDD) & Archive, backups & 2PB \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Parallel File System}

\begin{requirement}{COMP-006}{Lustre or BeeGFS deployment with 40GB/s aggregate throughput}

\begin{requirement}{COMP-007}{GPUDirect Storage support for direct GPU memory access}

\begin{itemize}
    \item Metadata servers: 2× redundant with SSD storage
    \item Object storage servers: 8× with 24 HDDs each
    \item High-speed interconnect: InfiniBand EDR/HDR
    \item Automatic tiering between hot/warm/cold storage
\end{itemize}

\section{Software Stack}

\subsection{Operating System and Virtualisation}

\begin{itemize}
    \item \textbf{Base OS}: Rocky Linux 9 or Ubuntu 22.04 LTS
    \item \textbf{Container Platform}: Docker with NVIDIA Container Toolkit
    \item \textbf{Orchestration}: Kubernetes for workload management
    \item \textbf{GPU Scheduling}: NVIDIA Multi-Instance GPU (MIG) support
\end{itemize}

\subsection{Rendering and Visualisation Software}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}lX@{}}
\toprule
\textbf{Software} & \textbf{Purpose} \\
\midrule
Unreal Engine 5.4+ & Primary real-time rendering engine with nDisplay \\
Unity 2024 LTS & Alternative renderer with MiddleVR support \\
NVIDIA Omniverse & Collaborative design and ray-traced rendering \\
OpenXR Runtime & VR/AR application compatibility \\
Custom Cluster Manager & Synchronisation and viewpoint management \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Development and Simulation Tools}

\begin{itemize}
    \item CUDA Toolkit and cuDNN for GPU computing
    \item OpenCL for vendor-agnostic compute
    \item TensorFlow/PyTorch for AI/ML workloads
    \item Ansys/COMSOL for engineering simulations
    \item ParaView for scientific visualisation
\end{itemize}

\section{Synchronisation and Timing}

\subsection{Hardware Synchronisation}

\begin{requirement}{COMP-008}{NVIDIA Quadro Sync II or successor for frame-lock across all GPUs}

\begin{requirement}{COMP-009}{PTP (Precision Time Protocol) grandmaster clock for sub-microsecond timing}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=0.8]
    % Sync cards
    \foreach \x in {0,3,6} {
        \draw[fill=dreamlabAccent!30] (\x,2) rectangle (\x+2,3);
        \node at (\x+1,2.5) {\small Sync Card};
    }

    % Connections
    \draw[dreamlabSecondary, ultra thick] (1,2) -- (1,1) -- (7,1) -- (7,2);
    \draw[dreamlabSecondary, ultra thick] (4,2) -- (4,1);

    % Master clock
    \draw[fill=dreamlabPrimary!30] (3,0) rectangle (5,0.8);
    \node at (4,0.4) {\small PTP Clock};
    \draw[dreamlabPrimary, thick] (4,0.8) -- (4,1);

    \node[below] at (4,-0.5) {\textbf{Distributed Frame Synchronisation}};
\end{tikzpicture}
\caption{Multi-GPU synchronisation architecture}
\end{figure}

\subsection{Software Synchronisation}

\begin{itemize}
    \item Custom frame-pacing algorithms for multi-viewer sequencing
    \item Predictive rendering to compensate for tracking latency
    \item Asynchronous spacewarp for missed frame compensation
    \item Time-stamped event logging for debugging
\end{itemize}

\section{Compute Performance Targets}

\subsection{Rendering Benchmarks}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}lXr@{}}
\toprule
\textbf{Metric} & \textbf{Requirement} & \textbf{Target} \\
\midrule
Single viewpoint FPS & 120 Hz minimum & 240 Hz \\
Total viewpoints & 36 (6 users × 6 walls) & 48 \\
Scene complexity & 10M polygons & 100M polygons \\
Ray tracing & Real-time GI & Path traced GI \\
Latency & <20ms & <10ms \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Simulation Capabilities}

\begin{itemize}
    \item Real-time physics: 1M rigid bodies at 60Hz
    \item Fluid dynamics: 10M particles SPH simulation
    \item AI agents: 1000+ autonomous entities
    \item Volumetric rendering: 1024³ voxel grids
\end{itemize}

\section{Resilience and Redundancy}

\subsection{Failover Capabilities}

\begin{requirement}{COMP-010}{N+1 redundancy for compute nodes}

\begin{requirement}{COMP-011}{Automatic workload migration on node failure}

\begin{itemize}
    \item Hot spare GPU nodes ready for immediate deployment
    \item Redundant network paths with automatic failover
    \item Distributed rendering allowing graceful degradation
    \item Snapshot-based recovery for system state
\end{itemize}

\subsection{Monitoring and Management}

\begin{itemize}
    \item \textbf{DCIM}: Data centre infrastructure management
    \item \textbf{GPU Monitoring}: NVIDIA DCGM for GPU health
    \item \textbf{Performance}: Grafana dashboards with Prometheus
    \item \textbf{Alerting}: PagerDuty integration for critical events
\end{itemize}

\section{Power and Cooling Requirements}

\subsection{Power Specifications}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{@{}lXr@{}}
\toprule
\textbf{Component} & \textbf{Configuration} & \textbf{Power Draw} \\
\midrule
GPU Nodes & 4× nodes with 8 GPUs each & 12kW \\
Storage Servers & 8× storage nodes & 4kW \\
Network Equipment & Switches, routers & 2kW \\
Auxiliary Systems & Management, backup & 2kW \\
\midrule
\textbf{Total} & & \textbf{20kW} \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Cooling Requirements}

\begin{itemize}
    \item Minimum 70kW cooling capacity (with N+1 redundancy)
    \item Hot/cold aisle containment
    \item Liquid cooling ready for future GPU upgrades
    \item Target PUE: <1.3
\end{itemize}

\section{Integration Considerations}

\subsection{Tracking System Interface}

\begin{requirement}{COMP-012}{Sub-millisecond latency for tracking data ingestion}

\begin{requirement}{COMP-013}{Support for VRPN and OpenXR tracking protocols}

\subsection{Audio System Interface}

\begin{itemize}
    \item Low-latency audio pipeline (<10ms)
    \item Spatial audio object positioning via OSC
    \item Hardware audio clock synchronisation
\end{itemize}

\subsection{Volumetric Capture Pipeline}

\begin{itemize}
    \item Direct camera-to-GPU data paths
    \item Real-time point cloud generation
    \item Mesh reconstruction at 30fps
    \item Network streaming of volumetric data
\end{itemize}

\section{Future Expansion}

\subsection{Scalability Provisions}

\begin{itemize}
    \item Rack space for 100\% capacity expansion
    \item Power and cooling headroom for additional nodes
    \item Network infrastructure supporting 400GbE upgrade
    \item Software architecture supporting distributed rendering
\end{itemize}

\subsection{Emerging Technology Support}

\begin{itemize}
    \item Quantum computing interface provisions
    \item Neuromorphic computing exploration
    \item AI accelerator integration capability
    \item Photonic computing research pathway
\end{itemize}

\begin{center}
\begin{tikzpicture}
\node[rectangle, draw=dreamlabPrimary, fill=dreamlabLight!20, text width=14cm, inner sep=15pt, rounded corners] {
\centering
\textbf{\large\color{dreamlabPrimary}Compute Infrastructure Summary}\\[0.5cm]
Our compute specification delivers unprecedented rendering power through a carefully architected cluster of 32+ cutting-edge GPUs, high-speed networking, and massive storage capacity. This infrastructure ensures smooth, photorealistic experiences for all users while maintaining the headroom for complex simulations and future expansion.
};
\end{tikzpicture}
\end{center}